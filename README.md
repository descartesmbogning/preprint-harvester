# ğŸ§¬ Preprint Harvester

### **Crossref Ã— DataCite Ã— OpenAlex â€” Unified Metadata Harvester for Preprint Servers**

**Author:** .....  
**License:** MIT  
**Status:** Public Release v1.0

---

## â­ Overview

**Preprint Harvester** is a unified Python toolkit designed to automatically collect preprint metadata across **Crossref**, **DataCite**, and **OpenAlex**.

The toolkit reads a **Google Sheetâ€“like rules table** that specifies how each preprint server should be queried, depending on:

* DOI prefixes  
* Crossref group titles  
* Members  
* Institution names  
* Domain-based URL patterns  
* DataCite client IDs  
* OpenAlex source IDs  

It supports:

âœ” Multi-provider harvesting  
âœ” Automatic output per server  
âœ” Date-range filtering  
âœ” Dry-run mode (no API calls)  
âœ” Export to Parquet and CSV  
âœ” Summary report generation  
âœ” Reproducible metadata pipelines  

---

## ğŸ“Œ Key Features

### ğŸ” 1. Multi-backend harvesting

* **Crossref**: prefix / member / group-title / domain / DOI-first-token  
* **DataCite**: client-id + resource-type filters  
* **OpenAlex**: primary_location.source.id + optional preprint filter  

### ğŸ—‚ 2. Sheet-driven rules

Supports a full rule specification from a spreadsheet with columns:

```text
Field_server_name
include
rules
rule_1 â€¦ rule_7
prefixes
members
group_title
institution_name
primary_domain
primary_domain_extend
doi_prefix_first_token
client_id
source_id
```

### ğŸ“ 3. Reproducible outputs

For each server:

```text
/data/by_server/<SERVER_NAME>/
    server_YYYY-MM-DD_YYYY-MM-DD_crossref.parquet
    server_YYYY-MM-DD_YYYY-MM-DD_crossref.csv
    server_YYYY-MM-DD_YYYY-MM-DD_datacite.parquet
    server_YYYY-MM-DD_YYYY-MM-DD_openalex.parquet
```

Plus a global summary CSV.

### ğŸ§ª 4. Dry-run mode

Prints **exact API parameters** without calling the APIs.

### ğŸ›¡ 5. Compliant with API best practices

* Uses polite `mailto=`  
* Retries for rate limits  
* Cursor-based pagination  

---

# ğŸ“¦ Installation

### 1. Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/preprint-harvester.git
cd preprint-harvester
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. (Optional) Install in development mode

```bash
pip install -e .
```

---

# ğŸ“Š Input: Rules Sheet Format

The program expects a **CSV downloaded from Google Sheets**.

Example minimal structure:

| Field_server_name | include | rules              | client_id    | source_id     | prefixes   | group_title              | primary_domain |
| ----------------- | ------- | ------------------ | ------------ | ------------- | ---------- | ------------------------ | -------------- |
| Preprints.org     | yes     | prefix             |              |               | [10.20944] |                          |                |
| OSF Preprints     | yes     | group_title        |              |               |            | [Open Science Framework] |                |
| CERN Doc Server   | yes     | client_id/preprint | ["cern.cds"] |               |            |                          |                |
| engrxiv           | yes     | source_id          |              | ["s4306401442"] |            |                          |                |

Fields may contain: `"a,b"`, `"[a,b]"`, or a single value.

---

# ğŸš€ Usage

## ğŸ”§ Running the harvester

The main entry point is:

```text
scripts/run_harvest_from_sheet.py
```

or directly via Python:

```python
from preprint_harvester.harvesters import harvest_servers_from_rules_sheet

harvest_servers_from_rules_sheet(
    sheet_csv_path_or_url="rules.csv",
    date_start="2025-01-01",
    date_end="2025-01-15",
    mailto="your.email@domain.com",
    dry_run=False
)
```

---

# ğŸ§ª Dry Run (No API calls)

To print all constructed API parameters *without querying APIs*:

```python
harvest_servers_from_rules_sheet(
    "rules.csv",
    date_start="2025-01-01",
    date_end="2025-01-15",
    mailto="your.email@example.com",
    dry_run=True
)
```

You will see printed:

* Resolved Crossref filters  
* DataCite query + resource_type  
* OpenAlex API query  
* API URL examples  

---

# ğŸ” Example API Calls (Automatically printed in summary)

### **Crossref Example**

```text
https://api.crossref.org/works?filter=from-posted-date:2025-01-01T00:00:00,
until-posted-date:2025-01-15T23:59:59,prefix:10.20944&type:posted-content
&rows=1000&cursor=*&mailto=you@example.com
```

### **DataCite Example**

```text
https://api.datacite.org/dois
  ?client-id=cern.cds
  &resource-type-id=Preprint
  &query=registered:[2025-01-01 TO 2025-01-15]
  &page[size]=1000
  &page[cursor]=1
```

### **OpenAlex Example**

```text
https://api.openalex.org/works
  ?filter=type:preprint,
           primary_location.source.id:s4306402450,
           from_publication_date:2025-01-01,
           to_publication_date:2025-01-15
  &per-page=200
  &cursor=*
  &mailto=you@example.com
```

---

# ğŸ“ Output Structure

After running the harvester:

```text
data/
â””â”€â”€ by_server/
    â”œâ”€â”€ Preprints.org/
    â”‚   â”œâ”€â”€ Preprints.org_2025-01-01_2025-01-15_crossref.parquet
    â”‚   â””â”€â”€ Preprints.org_2025-01-01_2025-01-15_crossref.csv
    â”œâ”€â”€ OSF Preprints/
    â”œâ”€â”€ CERN Document Server/
    â””â”€â”€ engrxiv/
harvest_summary_2025-01-01_2025-01-15_real.csv
```

Each folder contains **Crossref**, **DataCite**, and/or **OpenAlex** results depending on rules.

---

# ğŸ§± Directory Layout

```text
preprint-harvester/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprint_harvester/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ harvesters.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_harvest_from_sheet.py
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ example_rules_sheet.csv
â”‚   â””â”€â”€ example_usage.ipynb
â”‚
â”œâ”€â”€ tests/
â”‚
â””â”€â”€ data/
```

---

# ğŸ“– Example Notebook

See:

```text
/examples/example_usage.ipynb
```

It includes:

* Loading rules  
* Running dry-run  
* Fetching Crossref, DataCite, OpenAlex  
* Merging results  
* Displaying metadata  

---

# ğŸ§ª Testing

Minimal example:

```bash
pytest tests/
```

You can add mock-based tests for:

* Parameter building  
* Filtering  
* Fake API responses  

---

# âš ï¸ Troubleshooting

### â— 1. Crossref returns 429 Too Many Requests

Solution:

* Reduce `rows_per_call`  
* Add sleep time via polite_sleep  
* Ensure correct `mailto=`  

### â— 2. DataCite returns zero results

Check:

* `client_id` spelling  
* date range  
* resource-type combination  

### â— 3. OpenAlex returns no preprints

Make sure:

* The source actually deposits preprints  
* Remove `only_preprints=True` to test  

---

# ğŸ¤ Contributing

All contributions are welcome!

1. Fork the repo  
2. Create a new branch  
3. Submit a pull request  

Please format code with:

```bash
black .
isort .
```

---

# ğŸ“š Citation

If you use this tool in research:

```text
...... (2025).
Preprint Harvester: Unified Crossrefâ€“DataCiteâ€“OpenAlex Metadata Harvester.
GitHub Repository.
```
